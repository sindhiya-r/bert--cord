{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ignite_train_GPT2_abstractive_summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1DsJKLSRFQlf1B8EeAXJsgh7gZPqxkY86",
      "authorship_tag": "ABX9TyO7wLoc8Th9WMwvmYhLHj3Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VincentK1991/BERT_summarization_1/blob/master/Ignite_train_GPT2_abstractive_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXgDUdFRu_re",
        "colab_type": "text"
      },
      "source": [
        "# 0. Using Ignite to train GPT2 summarization\n",
        "\n",
        "This notebook illustrate how to use Ignite Engine to train GPT2 for abstractive summarization. The goal here is to get a fine-tuned weight tensors of GPT2 that we will later use for abstractive summarization of biomedical science publication. The dataset is processed from this [kaggle](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge).\n",
        "\n",
        "To get a sense on how to train GPT2, and why it is done this way, and what an outcome looks like, read this [companion notebook here](https://github.com/VincentK1991/BERT_summarization_1/blob/master/Copy_of_BERTandGPT2_abstractive_summarization_Apr28_2020.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNrcr2dDCtMn",
        "colab_type": "text"
      },
      "source": [
        "# 1. installing Pytorch, Huggingface, check GPU, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvPhkIDyC01L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "06eb6805-70de-4761-cb2f-98143d3fa5a4"
      },
      "source": [
        "%cd '/content/drive/My Drive/Colab Notebooks/GPT-2/Ignite_training_Apr29_2020'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/GPT-2/Ignite_training_Apr29_2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avdy-vj3CpWG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "f820177f-3d99-497a-de4a-dee6c75634fc"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr 30 13:32:21 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_-LItnaC5cK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import timeit\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
        "\n",
        "\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rtn0hEYfC8F3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "outputId": "8ca9c4b4-ca12-4fe1-918c-eab84879fb4b"
      },
      "source": [
        "!pip install transformers==2.6.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==2.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/a0/32e3a4501ef480f7ea01aac329a716132f32f7911ef1c2fac228acc57ca7/transformers-2.6.0-py3-none-any.whl (540kB)\n",
            "\u001b[K     |████████████████████████████████| 542kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (3.0.12)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 74.0MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 64.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (1.18.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (4.38.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (2.21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (1.12.46)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 69.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (0.14.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (2020.4.5.1)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.46 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (1.15.46)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->transformers==2.6.0) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->transformers==2.6.0) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=997e036241a17b512a28f58f214693db961489d48c9ac459b290f35d4b8554bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.41 sentencepiece-0.1.86 tokenizers-0.5.2 transformers-2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuaV9ZAJC-Kz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "9a58815b-c178-4dec-bb25-443a4d4192e2"
      },
      "source": [
        "import transformers\n",
        "from transformers import GPT2Tokenizer, GPT2DoubleHeadsModel, AdamW\n",
        "load_model = False\n",
        "load_previous_weight = False\n",
        "resize_model = False\n",
        "print(transformers.__version__) # make sure it is 2.6.0"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnTP6BLaC__u",
        "colab_type": "text"
      },
      "source": [
        "# 2. Test load the GPT2DoubleHeadsModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvEoc11BDEG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = GPT2DoubleHeadsModel.from_pretrained('Apr29_2020_epoch1')\n",
        "load_model = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAududTBDUWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('Apr29_2020_epoch1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwbG84ZnDUfl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "5181f895-1f55-4912-ee18-a162cba23613"
      },
      "source": [
        "print(len(tokenizer), 'total length of vocab') # expect 50257"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50257 total length of vocab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrFs8jhpDfZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add a [CLS] to the vocabulary (we should train it also!)\n",
        "special_tokens = {'bos_token':'<|startoftext|>','eos_token':'<|endoftext|>','pad_token':'<pad>','additional_special_tokens':['<|keyword|>','<|summarize|>']}\n",
        "#special_tokens2 = {'bos_token':'<|startoftext|>','eos_token':'<|endoftext|>','keyword_token':'<|keyword|>','summary_token':'<|summarize|>'}\n",
        "tokenizer.add_special_tokens(special_tokens)\n",
        "#model.resize_token_embeddings(len(tokenizer))  # Update the model embeddings with the new vocabulary size\n",
        "# The newly token the last token of the vocabulary\n",
        "resize_model = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9PuDymbDhWi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "7bdf7ef5-b1c7-4d50-bd83-90cbea0cbe12"
      },
      "source": [
        "print(len(tokenizer), 'total length of vocab')\n",
        "print(tokenizer.bos_token_id, 'bos_token')\n",
        "print(tokenizer.eos_token_id, 'eos_token')\n",
        "print(tokenizer.pad_token_id, 'pad_token')  #token for <pad>, len of all tokens in the tokenizer\n",
        "print(tokenizer.additional_special_tokens_ids[0], 'keyword_token') #token for <|keyword|>\n",
        "print(tokenizer.additional_special_tokens_ids[1], 'summary_token') #token for <|summarize|>"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50261 total length of vocab\n",
            "50257 bos_token\n",
            "50256 eos_token\n",
            "50258 pad_token\n",
            "50259 keyword_token\n",
            "50260 summary_token\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJtOy9bhDjB7",
        "colab_type": "text"
      },
      "source": [
        "expected output\n",
        "\n",
        "50261 total length of vocab\n",
        "\n",
        "50257 bos_token\n",
        "\n",
        "50256 eos_token\n",
        "\n",
        "50258 pad_token\n",
        "\n",
        "50259 keyword_token\n",
        "\n",
        "50260 summary_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3CXzM7QDyke",
        "colab_type": "text"
      },
      "source": [
        "# 3. Load dataset and make dataloader\n",
        "\n",
        "The dataset is in the torch tensor format. It is bundled into a tuple of 5 items, which are \n",
        "  1. the input tokens. \n",
        "  2. the segment tokens. \n",
        "  3. the index for last token (this is used for multiple choice), \n",
        "  4. the language model expected output tokens, the masked [-100] is used to mask away part that model doesn't have to output.\n",
        "\n",
        "    - this 1-4 items come in a batch of 4, only one of these 4 is the correct keyword-summary pair. The other 3 are distractors.\n",
        "\n",
        "  5. the multiple choice label which one of the 4 item in the current batch is the correct choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcA4bG1rD1hk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_1 = torch.load('torch_trainFile_2_Apr29_2020.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6PLukpnHIvX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "18561a49-3d6e-4720-cefa-b2cf96bc9459"
      },
      "source": [
        "train_dataset_1[5]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[50257,   370,  7456,  ..., 50258, 50258, 50258],\n",
              "         [50257,   370,  7456,  ..., 50258, 50258, 50258],\n",
              "         [50257,   370,  7456,  ..., 50258, 50258, 50258],\n",
              "         [50257,   370,  7456,  ..., 50258, 50258, 50258]]),\n",
              " tensor([[50259, 50259, 50259,  ..., 50258, 50258, 50258],\n",
              "         [50259, 50259, 50259,  ..., 50258, 50258, 50258],\n",
              "         [50259, 50259, 50259,  ..., 50258, 50258, 50258],\n",
              "         [50259, 50259, 50259,  ..., 50258, 50258, 50258]]),\n",
              " tensor([337,  86, 335, 290]),\n",
              " tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
              "         [-100, -100, -100,  ..., -100, -100, -100],\n",
              "         [-100, -100, -100,  ..., -100, -100, -100],\n",
              "         [-100, -100, -100,  ..., -100, -100, -100]]),\n",
              " tensor([1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNdPPBBHD6bg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for count,i in enumerate(train_dataset_1[5][3][0]):\n",
        "  i = int(i)\n",
        "  if i == -100:\n",
        "    decode_i = 'masked'\n",
        "  else:\n",
        "    decode_i = tokenizer.decode(i)\n",
        "  print(count,int(i), decode_i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktH2QWhWEACq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train1_sampler = RandomSampler(train_dataset_1)\n",
        "train1_dataloader = DataLoader(train_dataset_1, sampler=train1_sampler, batch_size=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auETmY1_EIHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_dataset_1 = torch.load('torch_valFile_1_Apr29_2020.pt')\n",
        "val1_sampler = RandomSampler(val_dataset_1)\n",
        "val1_dataloader = DataLoader(val_dataset_1, sampler=val1_sampler, batch_size=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Veq6QluxEJ-H",
        "colab_type": "text"
      },
      "source": [
        "# 4. Test run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swD1sd1lEMtW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "10f2d5c1-0655-41f5-f4b7-7c15c9f21514"
      },
      "source": [
        "input_ids, token_type_ids, mc_token_ids, lm_labels, mc_labels = train_dataset_1[0]\n",
        "print(input_ids.shape)\n",
        "print(mc_token_ids.shape)\n",
        "print(lm_labels.shape)\n",
        "print(mc_labels.shape)\n",
        "print(token_type_ids.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 1024])\n",
            "torch.Size([4])\n",
            "torch.Size([4, 1024])\n",
            "torch.Size([1])\n",
            "torch.Size([4, 1024])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckERPHCBEXeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)\n",
        "optimizer = AdamW(model.parameters(),lr=3e-5,eps=1e-8, correct_bias=True)\n",
        "max_norm = 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YELzPPDYEes2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gradient_accumulation_steps = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvEM_4J0EfrP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "22294c55-d9e2-4b72-d743-f506fffb79df"
      },
      "source": [
        "total_steps = len(train1_dataloader)\n",
        "print('total step for learning rate scheduler = ',total_steps)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total step for learning rate scheduler =  32146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAdZyGWAElW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 50, num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Br_fbZKEnYB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "142c7b96-86e0-4343-dd10-d2d515699d0d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr 30 13:35:30 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    31W / 250W |   1145MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz1hKJF9IAHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_run = train_dataset_1[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-5IWsFkEpil",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "60c63a03-4ba9-4b86-8890-78264f378153"
      },
      "source": [
        "# Forward pass\n",
        "start = timeit.default_timer()\n",
        "model.train()\n",
        "optimizer.zero_grad()\n",
        "test_run = (item.to(device) for item in test_run)\n",
        "input_ids, token_type_ids, mc_token_ids, lm_labels, mc_labels = test_run\n",
        "#input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = input_ids.to(device), mc_token_ids.to(device), lm_labels.to(device), mc_labels.to(device), token_type_ids.to(device)\n",
        "outputs = model(input_ids = input_ids, mc_token_ids = mc_token_ids, mc_labels = mc_labels,lm_labels = lm_labels, token_type_ids = token_type_ids)\n",
        "lm_loss, mc_loss = outputs[0], outputs[1]\n",
        "#del input_ids, token_type_ids, mc_token_ids, lm_labels, mc_labels\n",
        "lm_coef = 2.0\n",
        "mc_coef = 1.0\n",
        "total_loss = lm_loss * lm_coef + mc_loss * mc_coef\n",
        "print('lm_loss = ',lm_loss.item())\n",
        "print('mc_loss = ',mc_loss.item())\n",
        "print('total_loss = ',total_loss.item())\n",
        "total_loss.backward()\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "optimizer.step()\n",
        "stop = timeit.default_timer()\n",
        "print('1 epoch takes {:.3f}'.format(stop - start),' sec')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lm_loss =  1.9656041860580444\n",
            "mc_loss =  0.0\n",
            "total_loss =  3.931208372116089\n",
            "1 epoch takes 0.868  sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7P2O341EwlO",
        "colab_type": "text"
      },
      "source": [
        "# 5. set up Ignite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx9-CtyFEzVH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "26f7834e-353a-4450-bf47-a4db037b58b8"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-ignite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/55/41e8a995876fd2ade29bdba0c3efefa38e7d605cb353c70f3173c04928b5/pytorch_ignite-0.3.0-py2.py3-none-any.whl (103kB)\n",
            "\r\u001b[K     |███▏                            | 10kB 33.3MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 30kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 71kB 3.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 81kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 92kB 3.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 102kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch-ignite) (1.5.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-ignite) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-ignite) (1.18.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwDRaDqpE1dk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ignite.engine import Engine, Events\n",
        "from ignite.metrics import MeanSquaredError, Loss, RunningAverage\n",
        "from ignite.handlers import ModelCheckpoint, EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irKe3Q4kE32u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_function(engine,batch):\n",
        "  #start = timeit.default_timer()\n",
        "  model.train()\n",
        "  #optimizer.zero_grad()\n",
        "  batch = (item.to(device) for item in batch)\n",
        "  input_ids, token_type_ids, mc_token_ids, lm_labels, mc_labels = batch\n",
        "  outputs = model(input_ids = input_ids, mc_token_ids = mc_token_ids, mc_labels = mc_labels,\n",
        "                  lm_labels = lm_labels, token_type_ids = token_type_ids)\n",
        "  lm_loss, mc_loss = outputs[0], outputs[1]\n",
        "  #del input_ids, token_type_ids, mc_token_ids, lm_labels, mc_labels\n",
        "  lm_coef = 2.0\n",
        "  mc_coef = 1.0\n",
        "  total_loss = lm_loss * lm_coef + mc_loss * mc_coef\n",
        "  total_loss = total_loss / gradient_accumulation_steps\n",
        "  total_loss.backward()\n",
        "  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "  if engine.state.iteration % gradient_accumulation_steps == 0:\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "  scheduler.step()\n",
        "  return lm_loss.item(),mc_loss.item(),total_loss.item()*gradient_accumulation_steps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwXHCB0iE9CZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_function(engine,batch):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    batch = (item.to(device) for item in batch)\n",
        "    input_ids, token_type_ids, mc_token_ids, lm_labels, mc_labels = batch\n",
        "    outputs = model(input_ids = input_ids, mc_token_ids = mc_token_ids, mc_labels = mc_labels,\n",
        "                  lm_labels = lm_labels, token_type_ids = token_type_ids)\n",
        "    lm_loss, mc_loss = outputs[0], outputs[1]\n",
        "    lm_coef = 2.0\n",
        "    mc_coef = 1.0\n",
        "    total_loss = lm_loss * lm_coef + mc_loss * mc_coef\n",
        "  return lm_loss.item(),mc_loss.item(),total_loss.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f389O7hdE_4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = Engine(process_function)\n",
        "evaluator = Engine(evaluate_function)\n",
        "\n",
        "training_history = {'lm_loss': [], 'mc_loss': [], 'total_loss': []}\n",
        "validation_history = {'lm_loss': [], 'mc_loss': [], 'total_loss': []}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MInwHrZFGxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RunningAverage(output_transform=lambda x: x[0]).attach(trainer, 'lm_loss')\n",
        "RunningAverage(output_transform=lambda x: x[1]).attach(trainer, 'mc_loss')\n",
        "RunningAverage(output_transform=lambda x: x[2]).attach(trainer, 'total_loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgsWi2JLFcjJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RunningAverage(output_transform=lambda x: x[0]).attach(evaluator, 'lm_loss')\n",
        "RunningAverage(output_transform=lambda x: x[1]).attach(evaluator, 'mc_loss')\n",
        "RunningAverage(output_transform=lambda x: x[2]).attach(evaluator, 'total_loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LYmHqQWFfyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@trainer.on(Events.ITERATION_COMPLETED(every=100))\n",
        "def print_trainer_logs(engine):\n",
        "    # try:\n",
        "    #   start\n",
        "    # except:\n",
        "    #   start = timeit.default_timer()\n",
        "    loss_LM = engine.state.metrics['lm_loss']\n",
        "    loss_NSP = engine.state.metrics['mc_loss']\n",
        "    combined_loss = engine.state.metrics['total_loss']\n",
        "    stop = timeit.default_timer()\n",
        "    print(\"Trainer Results - iteration {} - LM loss: {:.2f} MC loss: {:.2f} total loss: {:.2f} report time: {:.1f}\"\n",
        "    .format(engine.state.iteration, loss_LM, loss_NSP, combined_loss,stop))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKuE-HHLFgzv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "d628e891-dfce-4d43-b143-2a6fd41dc078"
      },
      "source": [
        "checkpointer = ModelCheckpoint('/content/drive/My Drive/Colab Notebooks/GPT-2/Ignite_training_Apr29_2020/GPT2_dir', 'GPT2_summarizer', n_saved=2, create_dir=True, save_as_state_dict=True,require_empty=False)\n",
        "trainer.add_event_handler(Events.ITERATION_COMPLETED(every=15000), checkpointer, {'epoch_2': model})\n",
        "trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpointer, {'epoch_2_done': model})"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ignite.engine.engine.RemovableEventHandle at 0x7f5d9820feb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8KPEwhEFyU3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "19fd0d0e-38ea-4490-9b3b-e9aeb10a365d"
      },
      "source": [
        "def print_logs(engine, dataloader, mode, history_dict):\n",
        "    evaluator.run(dataloader, max_epochs=1)\n",
        "    metrics = evaluator.state.metrics\n",
        "    avg_LM_loss = metrics['lm_loss']\n",
        "    avg_NSP_loss = metrics['mc_loss']\n",
        "    avg_total_loss = metrics['total_loss']\n",
        "    #avg_loss =  avg_bce + avg_kld\n",
        "    print(\n",
        "        mode + \" Results - Epoch {} - Avg lm_loss: {:.2f} Avg mc_loss: {:.2f} Avg total_loss: {:.2f}\"\n",
        "        .format(engine.state.epoch, avg_LM_loss, avg_NSP_loss, avg_total_loss))\n",
        "    for key in evaluator.state.metrics.keys():\n",
        "        history_dict[key].append(evaluator.state.metrics[key])\n",
        "\n",
        "trainer.add_event_handler(Events.EPOCH_COMPLETED, print_logs, val1_dataloader, 'Validation', validation_history)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ignite.engine.engine.RemovableEventHandle at 0x7f5d9820fa20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiuJJk_4F6_F",
        "colab_type": "text"
      },
      "source": [
        "# Run Ignite Engine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeUi69-rF38h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e = trainer.run(train1_dataloader, max_epochs=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5hZkBOcF-VC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "95e753bc-4e0b-49d7-e46d-0a54f4786259"
      },
      "source": [
        "# save the model and tokenizer configuration\n",
        "model.config.to_json_file('GPT2_dir/config.json')\n",
        "tokenizer.save_vocabulary('GPT2_dir')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Apr29_2020_epoch2/vocab.json', 'Apr29_2020_epoch2/merges.txt')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NYa6z-RRNs1",
        "colab_type": "text"
      },
      "source": [
        "# 6. Result\n",
        "\n",
        "epoch 1\n",
        "\n",
        "- lm loss = 1.96-2.00\n",
        "- mc loss = 0.0\n",
        "- lr 3x10^-8, max_norm = 1.0, gradient accumulation = 5\n",
        "\n",
        "\n",
        "---\n",
        "epoch 2\n",
        "\n",
        "- lm loss = 1.77\n",
        "- mc loss = 0.0\n",
        "- lr 3x10^-8, max_norm = 1.0, gradient accumulation = 10\n",
        "\n"
      ]
    }
  ]
}